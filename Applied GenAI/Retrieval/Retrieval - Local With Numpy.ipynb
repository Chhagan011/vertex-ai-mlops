{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2f048b",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FApplied+GenAI%2FRetrieval&file=Retrieval+-+Local+With+Numpy.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Retrieval/Retrieval%20-%20Local%20With%20Numpy.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FApplied%2520GenAI%2FRetrieval%2FRetrieval%2520-%2520Local%2520With%2520Numpy.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Retrieval/Retrieval%20-%20Local%20With%20Numpy.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Applied%20GenAI/Retrieval/Retrieval%20-%20Local%20With%20Numpy.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698e894-1333-42b9-aaac-d85f393a8898",
   "metadata": {},
   "source": [
    "# Retrieval - Local With Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ea5c4-15a9-47d6-be99-55e527c9fa47",
   "metadata": {
    "id": "od_UkDpvRmgD"
   },
   "source": [
    "---\n",
    "## Colab Setup\n",
    "\n",
    "When running this notebook in [Colab](https://colab.google/) or [Colab Enterprise](https://cloud.google.com/colab/docs/introduction), this section will authenticate to GCP (follow prompts in the popup) and set the current project for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4fc4b5-ded9-4998-91c5-3a739f93abe1",
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1683726184843,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "8UO9FnqyKBlF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8556f8-aba8-48e8-bb1e-78a7f8d5e292",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68869,
     "status": "ok",
     "timestamp": 1683726253709,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "N98-KK7LRkjm",
    "outputId": "09ec5008-0def-4e1a-c349-c598ee752f78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    !gcloud config set project {PROJECT_ID}\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314aff0-869c-470d-a7df-accc3d0c8f87",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs and API Enablement\n",
    "\n",
    "The clients packages may need installing in this environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d7541-201a-4b15-843f-10ceaf40026d",
   "metadata": {},
   "source": [
    "### Installs (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "63e2119e-e87d-4e1c-8443-7db5a118c379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name, min_version)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform', '1.69.0'),\n",
    "    ('numpy', 'numpy'),\n",
    "    ('sklearn', 'scikit-learn'),\n",
    "    ('psutil', 'psutil'),\n",
    "    ('GPUtil', 'GPUtil')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user\n",
    "    elif len(package) == 3:\n",
    "        if importlib.metadata.version(package[0]) < package[2]:\n",
    "            print(f'updating package {package[1]}')\n",
    "            install = True\n",
    "            !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d797c9-2cb6-4324-8bd4-22d36e71eef8",
   "metadata": {},
   "source": [
    "### API Enablement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae98c1e2-5e2f-4435-8404-7b5a387943ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670aa3f-2855-46f4-92fa-102f3602768c",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f30908a-8c84-4e1b-8376-7b4da7738d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "    IPython.display.display(IPython.display.Markdown(\"\"\"<div class=\\\"alert alert-block alert-warning\\\">\n",
    "        <b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. The previous cells do not need to be run again⚠️</b>\n",
    "        </div>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4dc5ac-5948-4712-a63a-a13653a4753e",
   "metadata": {
    "id": "appt8-yVRtJ1"
   },
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f5d4f-f85e-454d-bbf6-36c369d5afdb",
   "metadata": {
    "id": "63mx2EozRxFP"
   },
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258676f3-d059-4cab-9d98-1a9502b1c87b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2124,
     "status": "ok",
     "timestamp": 1683726390544,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "xzcoXjM5Rky5",
    "outputId": "b3bdcbc1-70d5-472e-aea2-42c74a42efde",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8b99e0-801b-4615-aa34-4338f102e54e",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683726390712,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "IxWrFtqYMfku",
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'applied-genai'\n",
    "EXPERIMENT = 'retrieval-numpy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79513291-1fa9-4793-865b-a04b4a97d3aa",
   "metadata": {
    "id": "LuajVwCiO6Yg"
   },
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91d2d7be-0aa0-4d8a-8bb6-85486fbd241d",
   "metadata": {
    "executionInfo": {
     "elapsed": 17761,
     "status": "ok",
     "timestamp": 1683726409304,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "LVC7zzSLRk2C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json, sys, time\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.cluster\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "import vertexai.language_models # for embeddings API\n",
    "import vertexai.generative_models # for Gemini Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec644e4-2d42-4e15-9773-6c08b4c7b018",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.69.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21071181-1138-45ee-828c-5a98bf0df696",
   "metadata": {
    "id": "EyAVFG9TO9H-"
   },
   "source": [
    "Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3011a2d7-e026-4015-a4af-5eac8420bf84",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1683726409306,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "L0RPE13LOZce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vertex ai clients\n",
    "vertexai.init(project = PROJECT_ID, location = REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806df85-71ff-440e-8a23-c1806e689a2d",
   "metadata": {},
   "source": [
    "---\n",
    "## Text & Embeddings For Examples\n",
    "\n",
    "This repository contains a [section for document processing (chunking)](../Chunking/readme.md) that includes an [example of processing a PDF with the Document AI Layout Parser](../Chunking/Process%20Documents%20-%20Document%20AI%20Layout%20Parser.ipynb).  The chunks of text from that workflow are stored with this repository and loaded by another companion workflow that augments the chunks with text embeddings: [Vertex AI Text Embeddings API](../Embeddings/Vertex%20AI%20Text%20Embeddings%20API.ipynb).\n",
    "\n",
    "The following code will load the version of the chunks that includes text embeddings and prepare it for a local example of retrival augmented generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0538b-1b96-4a2f-92c3-4ffc01c9d8a5",
   "metadata": {},
   "source": [
    "### Get The Documents\n",
    "\n",
    "If you are working from a clone of this notebooks [repository](https://github.com/statmike/vertex-ai-mlops) then the documents are already present. The following cell checks for the documents folder and if it is missing gets it (`git clone`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d660e6-c6d7-4a74-829f-6aeb8157c8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_dir = '../Embeddings/files/embeddings-api'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e94bcb5-c705-4524-8e22-afe4a879f4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents Found in folder `../Embeddings/files/embeddings-api`\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(local_dir):\n",
    "    print('Retrieving documents...')\n",
    "    parent_dir = os.path.dirname(local_dir)\n",
    "    temp_dir = os.path.join(parent_dir, 'temp')\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "    !git clone https://www.github.com/statmike/vertex-ai-mlops {temp_dir}/vertex-ai-mlops\n",
    "    shutil.copytree(f'{temp_dir}/vertex-ai-mlops/Applied GenAI/Embeddings/files/embeddings-api', local_dir)\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f'Documents are now in folder `{local_dir}`')\n",
    "else:\n",
    "    print(f'Documents Found in folder `{local_dir}`')             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336a06f-ad9a-48d3-9e0b-7bc0ceec181a",
   "metadata": {},
   "source": [
    "### Load The Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "563b8ad1-f53e-414a-a256-ba086d36e96b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(local_dir+'/chunk-embeddings.jsonl', 'r') as f:\n",
    "    chunks = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79406f-53be-43df-944a-fe3d5674376e",
   "metadata": {},
   "source": [
    "### Review A Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d50e4de-f7ba-4ee9-ad64-c360cfb2029a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['instance', 'predictions', 'status'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37fd0807-9c88-420d-a816-723164a2caef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]['instance']['chunk_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "428bd5a2-1f38-4b57-b538-b4e11d6a1744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# OFFICIAL BASEBALL RULES\n",
      "\n",
      "## Official Baseball Rules 2023 Edition\n",
      "\n",
      "### JOINT COMPETITION COMMITTEE\n",
      "\n",
      "|-|-|-|\n",
      "| Bill DeWitt | Whit Merrifield | Austin Slater |\n",
      "| Jack Flaherty | Bill Miller | John Stanton, Chair |\n",
      "| Tyler Glasnow | Dick Monfort | Tom Werner |\n",
      "| Greg Johnson | Mark Shapiro |  |\n",
      "\n",
      "Committee Secretary Paul V. Mifsud, Jr. Copyright © 2023 by the Office of the Commissioner of Baseball\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0]['instance']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd0e3b12-3aed-4f37-bcdc-82b052358408",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008681542240083218,\n",
       " 0.06999468058347702,\n",
       " 0.003673204220831394,\n",
       " 0.019888797774910927,\n",
       " 0.016285404562950134,\n",
       " 0.035664502531290054,\n",
       " 0.06200747936964035,\n",
       " 0.05597030743956566,\n",
       " 0.0034793149679899216,\n",
       " -0.024485772475600243]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]['predictions'][0]['embeddings']['values'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bffd2-4858-4c26-bbd0-6eb2f94d4250",
   "metadata": {},
   "source": [
    "### Prepare Chunk Structure\n",
    "\n",
    "Make a dictionary for each lookup of chunk content by chunk id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffcdb0a4-2c20-4157-9caa-f23e8b26e040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_chunks = {}\n",
    "for chunk in chunks:\n",
    "    content_chunks[chunk['instance']['chunk_id']] = chunk['instance']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf69ede4-5ce0-4b43-b08e-9973cb3f6355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# OFFICIAL BASEBALL RULES\\n\\n2023 Edition TM TM'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_chunks['c1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e75bc-911c-4f7f-9a9d-1323c0e646ef",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple Retrieval Augmented Generation (RAG): Local With Numpy!\n",
    "\n",
    "Embeddings can be used with math to measure similarity.  For deeper details into this checkout the companion workflow here: [The Math of Similarity](../Embeddings/The%20Math%20of%20Similarity.ipynb).  Retrieval systems handle the storage and math of similarity as a service.  For an overview of Google Cloud based solutions for retrieval check out [this companion series](../Retrieval/readme.md).\n",
    "\n",
    "The content below motivates retrieval with the embeddings that accompany the text chunks using a local vector database with brute force matching using [Numpy](https://numpy.org/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabeb152-824b-42ff-a478-4752f5f8bf14",
   "metadata": {},
   "source": [
    "### Vector DB With Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "684cfaf8-1455-4fe7-8ac1-2c9bee9ef66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_db = [\n",
    "    [\n",
    "        chunk['instance']['chunk_id'],\n",
    "        chunk['predictions'][0]['embeddings']['values'],\n",
    "    ]\n",
    "    for chunk in chunks\n",
    "]\n",
    "vector_index = np.array([row[1] for row in vector_db])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d84ee006-6068-4c76-aee7-f459e3ba278e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cf20428-b4c4-43af-8aeb-b5091af83bba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(867, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_index.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3d843-826f-4531-b71d-30347453d1f0",
   "metadata": {},
   "source": [
    "### Models: Embeddings, Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9b92c-466b-4f5d-a16d-f35d4647c7dc",
   "metadata": {},
   "source": [
    "Connect to models for text embeddings and text generation:\n",
    "\n",
    "Learn more about these model APIs:\n",
    "- [Vertex AI Text Embeddings API](../Embeddings/Vertex%20AI%20Text%20Embeddings%20API.ipynb)\n",
    "- [Vertex AI Gemini API](../Generate/Vertex%20AI%20Gemini%20API.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20fab15d-b663-414a-89d8-62fdcbb58bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedder = vertexai.language_models.TextEmbeddingModel.from_pretrained('text-embedding-004')\n",
    "llm = vertexai.generative_models.GenerativeModel(\"gemini-1.5-flash-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1cdf0c-5ab5-4d70-95fc-132505b37292",
   "metadata": {},
   "source": [
    "Define a question that is the start of our prompt to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "519d1d3a-c5f7-4321-a697-ef50942a1718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What are the dimensions of a base?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f604942-5a78-4528-af96-187bab952841",
   "metadata": {},
   "source": [
    "Get an ungrounded response to the question with the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3d40e7a-dbdc-4f08-83c7-7c77ad32feee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term \"base\" can have different meanings depending on the context. Please clarify what you mean by \"base\". Here are some possible interpretations:\n",
      "\n",
      "**In geometry:**\n",
      "\n",
      "* **Base of a triangle:**  A side of a triangle, often the one perpendicular to the altitude (height).\n",
      "* **Base of a rectangle or parallelogram:**  Any one of the sides of the shape.\n",
      "* **Base of a pyramid or cone:**  The flat face on which the shape rests.\n",
      "* **Base of a cylinder:**  The circular face on which the cylinder rests.\n",
      "\n",
      "**In other contexts:**\n",
      "\n",
      "* **Base of a number system:** The number of unique digits used in a number system (e.g., base-10 for decimal numbers).\n",
      "* **Base of a chemical compound:** The atom or group of atoms that forms the core structure of the molecule. \n",
      "\n",
      "**Please provide more information about the context of the \"base\" you are referring to, and I can help you determine its dimensions.** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm.generate_content(question).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca5543-ce0c-4794-acb2-b2fc550f0c2f",
   "metadata": {},
   "source": [
    "Get an embedding for the question to use in retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "037c3260-c139-4d1b-8838-a571148d0285",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.026682045310735703,\n",
       " 0.011593513190746307,\n",
       " 0.028523651883006096,\n",
       " -0.0017065361607819796,\n",
       " 0.01946176588535309,\n",
       " 0.0031198114156723022,\n",
       " 0.07915323227643967,\n",
       " -0.005078596994280815,\n",
       " -0.006295712199062109,\n",
       " 0.04943541809916496]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "question_embedding[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544eabbe-0423-4d20-8dd6-d1fa4867f609",
   "metadata": {},
   "source": [
    "### Retrieval: Matching With Numpy\n",
    "\n",
    "Use dot product to calculate similarity and find matches for a query embedding.  Why dot product?  Check out the companion workflow: [The Math of Similarity](../Embeddings/The%20Math%20of%20Similarity.ipynb)\n",
    "\n",
    "> **NOTE:**  This will calculate the similarity for all embeddings vectors stored in the local vector db which is just a Numpy array here.  This is very fast because there are <200 embeddings vectors.  As this scales it would be better to consider a solution that searches a subset of embeddings.  More details on retrieval solutions can be found in [Retrieval](../Retrieval/readme.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c86827f8-ea68-4d67-8dc6-c8ac433eb5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(26, 0.5033481946111171),\n",
       " (40, 0.5126844935129918),\n",
       " (836, 0.5244194362041271),\n",
       " (36, 0.5724333016720691),\n",
       " (38, 0.5843799337008113)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = np.dot(question_embedding, vector_index.T)\n",
    "matches = np.argsort(similarity)[-5:].tolist()\n",
    "matches = [(match, similarity[match]) for match in matches]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "96800d03-70e8-4a7d-bf41-fb6ae579787f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match 1 (0.50) is chunk c31:\n",
      "# 2.00-THE PLAYING FIELD\n",
      "\n",
      "## 2.01 Layout of the Field\n",
      "\n",
      "When location of home base is determined, with a steel tape measure 127 feet, 3\\frac{3}{8} inches in desired direction to establish second base. From home base, measure 90 feet toward first base; from second base, measure 90 feet toward first base; the intersection of these lines establishes first base. From home base, measure 90 feet toward third base; from second base, measure 90 feet toward third base; the intersection of these lines establishes third base.\n",
      "###################################################\n",
      "Match 2 (0.51) is chunk c40:\n",
      "# Rule 2.03 to 2.05\n",
      "\n",
      "## 2.03 The Bases\n",
      "\n",
      "First, second and third bases shall be marked by white canvas or rubber-covered bags, securely attached to the ground as indicated in Diagram 2. The first and third base bags shall be entirely within the infield. The second base bag shall be centered on second base. The bags shall be 18 inches square, not less than three nor more than five inches thick, and filled with soft material.\n",
      "###################################################\n",
      "Match 3 (0.52) is chunk c838:\n",
      "# APPENDICES\n",
      "\n",
      "## Appendix 2\n",
      "\n",
      "Diagram No. 2 Layout at Home Plate, 1st, 2nd, and 3rd Bases 18\" A 18\" 90° LAYOUT AT SECOND BASE FOR LAYOUT AT PITCHER'S PLATE SEE DIAGRAM NO. 3 90° 6\" 17\" 6\" D E 3'0\" 3'0\" 4'0\" 4 C 43\" LAYOUT AT HOME BASE DIAGRAM NO. 2 LEGEND A 1st, 2nd, 3rd BASES BATTER'S BOX B B C CATCHER'S BOX D HOME BASE E PITCHER'S PLATE Rev2023RW 161 90° 4 FOUL LINE LAYOUT AT FIRST BASE\n",
      "###################################################\n",
      "Match 4 (0.57) is chunk c39:\n",
      "# 2.00-THE PLAYING FIELD\n",
      "\n",
      "## 2.02 Home Base\n",
      "\n",
      "It shall be set in the ground with the point at the intersection of the lines extending from home base to first base and to third base; with the 17-inch edge facing the pitcher's plate, and the two 12-inch edges coinciding with the first and third base lines. The top edges of home base shall be beveled and the base shall be fixed in the ground level with the ground surface. (See drawing D in Appendix 2.) 3\n",
      "###################################################\n",
      "Match 5 (0.58) is chunk c38:\n",
      "# 2.00-THE PLAYING FIELD\n",
      "\n",
      "## 2.02 Home Base\n",
      "\n",
      "Home base shall be marked by a five-sided slab of whitened rubber. It shall be a 17-inch square with two of the corners removed so that one edge is 17 inches long, two adjacent sides are 8\\frac{1}{2} inches and the remaining two sides are 12 inches and set at an angle to make a point.\n",
      "###################################################\n"
     ]
    }
   ],
   "source": [
    "for m, match in enumerate(matches):\n",
    "    print(f\"Match {m+1} ({match[1]:.2f}) is chunk {vector_db[match[0]][0]}:\\n{content_chunks[vector_db[match[0]][0]]}\\n###################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7a1da-7246-497b-9e01-60e0d992ac14",
   "metadata": {},
   "source": [
    "### Generation: Q&A With Gemini Grounded With RAG\n",
    "\n",
    "Provide the matched chunks of text along with the question as a prompt to a generative model for a grounded answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de3fec-9fc4-4a59-893a-b44861fb98e6",
   "metadata": {},
   "source": [
    "#### Prompt Building Function\n",
    "\n",
    "Use the matching chunks as context for the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8a903d8-a05f-46db-91b5-d33f8f1b5024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prompt(question, top_n = 5):\n",
    "    # get embedding for question\n",
    "    question_embedding = embedder.get_embeddings([question])[0].values\n",
    "    # get top_n matches:\n",
    "    similarity = np.dot(question_embedding, vector_index.T)\n",
    "    matches = np.argsort(similarity)[-top_n:].tolist()\n",
    "    matches = [[match, similarity[match]] for match in matches]\n",
    "    # construct prompt:\n",
    "    prompt = ''\n",
    "    for m, match in enumerate(matches):\n",
    "        prompt += f\"Context {m+1}:\\n{content_chunks[vector_db[match[0]][0]]}\\n\\n\"\n",
    "    prompt += f'Answer the following question using the provided contexts:\\n{question}'\n",
    "    \n",
    "    return matches, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "045214b6-cbf8-4118-a582-fd3041c60c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context 1:\n",
      "# 2.00-THE PLAYING FIELD\n",
      "\n",
      "## 2.01 Layout of the Field\n",
      "\n",
      "When location of home base is determined, with a steel tape measure 127 feet, 3\\frac{3}{8} inches in desired direction to establish second base. From home base, measure 90 feet toward first base; from second base, measure 90 feet toward first base; the intersection of these lines establishes first base. From home base, measure 90 feet toward third base; from second base, measure 90 feet toward third base; the intersection of these lines establishes third base.\n",
      "\n",
      "Context 2:\n",
      "# Rule 2.03 to 2.05\n",
      "\n",
      "## 2.03 The Bases\n",
      "\n",
      "First, second and third bases shall be marked by white canvas or rubber-covered bags, securely attached to the ground as indicated in Diagram 2. The first and third base bags shall be entirely within the infield. The second base bag shall be centered on second base. The bags shall be 18 inches square, not less than three nor more than five inches thick, and filled with soft material.\n",
      "\n",
      "Context 3:\n",
      "# APPENDICES\n",
      "\n",
      "## Appendix 2\n",
      "\n",
      "Diagram No. 2 Layout at Home Plate, 1st, 2nd, and 3rd Bases 18\" A 18\" 90° LAYOUT AT SECOND BASE FOR LAYOUT AT PITCHER'S PLATE SEE DIAGRAM NO. 3 90° 6\" 17\" 6\" D E 3'0\" 3'0\" 4'0\" 4 C 43\" LAYOUT AT HOME BASE DIAGRAM NO. 2 LEGEND A 1st, 2nd, 3rd BASES BATTER'S BOX B B C CATCHER'S BOX D HOME BASE E PITCHER'S PLATE Rev2023RW 161 90° 4 FOUL LINE LAYOUT AT FIRST BASE\n",
      "\n",
      "Context 4:\n",
      "# 2.00-THE PLAYING FIELD\n",
      "\n",
      "## 2.02 Home Base\n",
      "\n",
      "It shall be set in the ground with the point at the intersection of the lines extending from home base to first base and to third base; with the 17-inch edge facing the pitcher's plate, and the two 12-inch edges coinciding with the first and third base lines. The top edges of home base shall be beveled and the base shall be fixed in the ground level with the ground surface. (See drawing D in Appendix 2.) 3\n",
      "\n",
      "Context 5:\n",
      "# 2.00-THE PLAYING FIELD\n",
      "\n",
      "## 2.02 Home Base\n",
      "\n",
      "Home base shall be marked by a five-sided slab of whitened rubber. It shall be a 17-inch square with two of the corners removed so that one edge is 17 inches long, two adjacent sides are 8\\frac{1}{2} inches and the remaining two sides are 12 inches and set at an angle to make a point.\n",
      "\n",
      "Answer the following question using the provided contexts:\n",
      "What are the dimensions of a base?\n"
     ]
    }
   ],
   "source": [
    "matches, prompt = get_prompt(question) \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ebc160-511f-471e-9b50-16dd109fa461",
   "metadata": {},
   "source": [
    "### Grounded Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1ce640c8-81a6-4a9c-9d9e-bbe622bb43c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Context 3, the bases are **18 inches square**, with a thickness of **between 3 and 5 inches**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = llm.generate_content(prompt).text\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d6c82-6166-4065-8a60-1e74d08e5bf0",
   "metadata": {},
   "source": [
    "---\n",
    "## Profiling Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5c5d2-6280-43bf-8433-56d4e396dfb7",
   "metadata": {},
   "source": [
    "### Size Of Objects\n",
    "\n",
    "The design above involves three objects:\n",
    "- `vector_db` - a Python list of list objects that each contain a chunk_id and the embedding vector for the chunk\n",
    "- `vector_index` - a numpy array of rows for each embedding vector\n",
    "- `content_chunks` - a Python dict that has keys for each chunk_id and values are the text of the chunk\n",
    "\n",
    "These are used by finding the index of matching embeddings from the `vector_index` and then looking up the cooresponding chunk_id in `vector_db` before finally retrieving the text of the chunk from `content_chunks`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6c60b-b23e-41ef-aedc-ede695b6e06d",
   "metadata": {},
   "source": [
    "#### Object: `vector_db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43a86991-af81-434f-a6d4-4acdb9c18145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1eedc2f8-6a4f-4335-96b1-022ec55aa4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "116c4524-963c-4b88-a0bd-86da9ab7a9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7832"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in bytes\n",
    "sys.getsizeof(vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0472d48f-5538-48bb-bf4a-43b5d4ee2fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00746917724609375"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in megabytes\n",
    "sys.getsizeof(vector_db)/ (1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4355f6-7cad-46ae-b5f8-4ee21dc4e9ab",
   "metadata": {},
   "source": [
    "#### Object: `vector_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5a6e98f-ff33-43b0-9708-1eb324438527",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2a964af-6d36-42a0-a11f-61e7fa019452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(867, 768)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48bb5d32-8b45-4a4a-86b2-3a5137a66f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5326976"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in bytes\n",
    "sys.getsizeof(vector_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85862542-f5e2-4047-8e9c-98f1b99df17c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0802001953125"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in megabytes\n",
    "sys.getsizeof(vector_index)/ (1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62cc570-507d-4854-9f40-45e1756c36ab",
   "metadata": {},
   "source": [
    "#### Object: `content_chunks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76d42354-57c3-4236-96f9-bbdbc64e945e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(content_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c2b451a-4df9-4d1e-b758-c8a9c4d094cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eec6ff5c-fda8-483a-844e-6d8145ed3e32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36960"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in bytes\n",
    "sys.getsizeof(content_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2ec63cb-3ff7-4d76-83a9-32fcd0c882d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.035247802734375"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in megabytes\n",
    "sys.getsizeof(content_chunks)/ (1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ac782-3788-43a9-8ef4-bab23c89bc67",
   "metadata": {},
   "source": [
    "### Local Compute Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee08e2ce-1a32-4f54-9253-a85c38bf4d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Count: 4\n",
      "CPU Frequency: 2199.998 MHz\n"
     ]
    }
   ],
   "source": [
    "# Get CPU count\n",
    "cpu_count = psutil.cpu_count(logical=True)  # Includes logical cores (hyperthreading)\n",
    "print(f\"CPU Count: {cpu_count}\")\n",
    "\n",
    "# Get CPU frequency\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "print(f\"CPU Frequency: {cpu_freq.current} MHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1b94670a-cdeb-4f65-97fd-8af2ab4e288c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 31.36 GB\n",
      "Used Memory: 14.50 GB\n",
      "Available Memory: 16.42 GB\n",
      "Memory Percentage Used: 47.6%\n"
     ]
    }
   ],
   "source": [
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total Memory: {mem.total / (1024**3):.2f} GB\")\n",
    "print(f\"Used Memory: {mem.used / (1024**3):.2f} GB\")\n",
    "print(f\"Available Memory: {mem.available / (1024**3):.2f} GB\")\n",
    "print(f\"Memory Percentage Used: {mem.percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66d9c272-6100-48f4-8821-3e17736d92fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found.\n"
     ]
    }
   ],
   "source": [
    "# Get all available GPUs\n",
    "gpus = GPUtil.getGPUs()\n",
    "\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU Name: {gpu.name}\")\n",
    "        print(f\"GPU Memory Total: {gpu.memoryTotal} MB\")\n",
    "        print(f\"GPU Memory Used: {gpu.memoryUsed} MB\")\n",
    "        print(f\"GPU Memory Free: {gpu.memoryFree} MB\")\n",
    "        print(f\"GPU Load: {gpu.load*100}%\")\n",
    "else:\n",
    "    print(\"No GPUs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d617671-24dc-46a2-bf9c-c685c4be0343",
   "metadata": {},
   "source": [
    "### Timing Sequential Operations\n",
    "\n",
    "Get the timing for different sequential matching requests loads: 1, 10, 100, 1000, 10000, 100000, ...\n",
    "\n",
    "Break this down by the tasks:\n",
    "- Get Embedding Vector For Question\n",
    "- Get Matching Chunks\n",
    "- Construct Prompt From Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e1086-2e2e-42bb-830a-ca3af509bc22",
   "metadata": {},
   "source": [
    "#### Get Embedding Vector For Question: API\n",
    "\n",
    "This test sequential request.  The Text Embeddings API has many option for asynchronous and multi-instance request that could also be used for efficiency.  See more in [Vertex AI Text Embeddings API](../Embeddings/Vertex%20AI%20Text%20Embeddings%20API.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ea67402-6885-4889-bd7e-7936383c8c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for n = 1: 0.083227 seconds\n",
      "Execution time for n = 10: 0.887779 seconds\n",
      "Execution time for n = 100: 5.782034 seconds\n",
      "Execution time for n = 1000: 58.156775 seconds\n"
     ]
    }
   ],
   "source": [
    "embed_time = []\n",
    "for x in range(4):\n",
    "    n = 10**x\n",
    "    start_time = time.time()\n",
    "    for i in range(n):\n",
    "        # get embedding for question\n",
    "        question_embedding = embedder.get_embeddings([question])[0].values\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time for n = {n}: {execution_time:.6f} seconds\")\n",
    "    embed_time.append(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0707b-62ab-49d9-ac1f-f5ce236f9da2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get Matching Chunks: Python + Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eb19fde8-87b0-463d-9b17-7d152e37078f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for n = 1: 0.000586 seconds\n",
      "Execution time for n = 10: 0.004576 seconds\n",
      "Execution time for n = 100: 0.041530 seconds\n",
      "Execution time for n = 1000: 0.409031 seconds\n"
     ]
    }
   ],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "\n",
    "match_time = []\n",
    "for x in range(4):\n",
    "    n = 10**x\n",
    "    start_time = time.time()\n",
    "    for i in range(n):\n",
    "        # get top_n matches:\n",
    "        top_n = 10\n",
    "        similarity = np.dot(question_embedding, vector_index.T)\n",
    "        matches = np.argsort(similarity)[-top_n:].tolist()\n",
    "        matches = [[match, similarity[match]] for match in matches]\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time for n = {n}: {execution_time:.6f} seconds\")\n",
    "    match_time.append(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57828f-d48f-4766-b0ca-2a53bbab2cd5",
   "metadata": {},
   "source": [
    "#### Construct Prompt Form Matches: Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9fd054b4-b0c7-40f1-bdb6-137f6090a8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for n = 1: 0.000063 seconds\n",
      "Execution time for n = 10: 0.000231 seconds\n",
      "Execution time for n = 100: 0.002236 seconds\n",
      "Execution time for n = 1000: 0.022976 seconds\n"
     ]
    }
   ],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "\n",
    "# get top_n matches:\n",
    "top_n = 10\n",
    "similarity = np.dot(question_embedding, vector_index.T)\n",
    "matches = np.argsort(similarity)[-top_n:].tolist()\n",
    "matches = [[match, similarity[match]] for match in matches]\n",
    "\n",
    "prompt_time = []\n",
    "for x in range(4):\n",
    "    n = 10**x\n",
    "    start_time = time.time()\n",
    "    for i in range(n):\n",
    "        # construct prompt:\n",
    "        prompt = ''\n",
    "        for m, match in enumerate(matches):\n",
    "            prompt += f\"Context {m+1}:\\n{content_chunks[vector_db[match[0]][0]]}\\n\\n\"\n",
    "        prompt += f'Answer the following question using the provided contexts:\\n{question}'\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time for n = {n}: {execution_time:.6f} seconds\")\n",
    "    prompt_time.append(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441d2aa-6f7f-4765-9a76-486443b97b7b",
   "metadata": {},
   "source": [
    "#### Combined Local Process of Matching, Prompt Construction:\n",
    "\n",
    "Leave out the embeddings requests where are API wait time to the local processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d495cb2b-2bba-428f-a96c-92e50ae98795",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for n = 1: 0.001533 seconds\n",
      "Execution time for n = 10: 0.017490 seconds\n",
      "Execution time for n = 100: 0.126925 seconds\n",
      "Execution time for n = 1000: 0.491893 seconds\n",
      "Execution time for n = 10000: 4.189425 seconds\n",
      "Execution time for n = 100000: 44.745094 seconds\n"
     ]
    }
   ],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "\n",
    "combined_time = []\n",
    "for x in range(6):\n",
    "    n = 10**x\n",
    "    start_time = time.time()\n",
    "    for i in range(n):\n",
    "\n",
    "        # get top_n matches:\n",
    "        top_n = 10\n",
    "        similarity = np.dot(question_embedding, vector_index.T)\n",
    "        matches = np.argsort(similarity)[-top_n:].tolist()\n",
    "        matches = [[match, similarity[match]] for match in matches]\n",
    "        \n",
    "        # construct prompt:\n",
    "        prompt = ''\n",
    "        for m, match in enumerate(matches):\n",
    "            prompt += f\"Context {m+1}:\\n{content_chunks[vector_db[match[0]][0]]}\\n\\n\"\n",
    "        prompt += f'Answer the following question using the provided contexts:\\n{question}'\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time for n = {n}: {execution_time:.6f} seconds\")\n",
    "    combined_time.append(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff490ed7-4b64-4002-9bb0-eb9ce7b148d4",
   "metadata": {},
   "source": [
    "### Profile Sequential Operations Timing\n",
    "\n",
    "Now collect the individual timings for local operations and review the profile of timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ad7599ab-1145-4ee0-9c23-14274ec9e961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "\n",
    "combined_time_profile = []\n",
    "\n",
    "n = 10000\n",
    "\n",
    "for i in range(n):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # get top_n matches:\n",
    "    top_n = 10\n",
    "    similarity = np.dot(question_embedding, vector_index.T)\n",
    "    matches = np.argsort(similarity)[-top_n:].tolist()\n",
    "    matches = [[match, similarity[match]] for match in matches]\n",
    "\n",
    "    # construct prompt:\n",
    "    prompt = ''\n",
    "    for m, match in enumerate(matches):\n",
    "        prompt += f\"Context {m+1}:\\n{content_chunks[vector_db[match[0]][0]]}\\n\\n\"\n",
    "    prompt += f'Answer the following question using the provided contexts:\\n{question}'\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    combined_time_profile.append(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b06eb689-e5c9-4afc-a20d-8208acbfc109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for all requests: 5.589185 seconds\n",
      "Average time per request: 0.000559 seconds\n",
      "Range of time across all requests: 0.016161 seconds\n",
      "99th percentile of request times: 0.001382 seconds\n"
     ]
    }
   ],
   "source": [
    "# Total time for all requests\n",
    "total_time = sum(combined_time_profile)\n",
    "print(f\"Total time for all requests: {total_time:.6f} seconds\")\n",
    "\n",
    "# Average time per request\n",
    "average_time = total_time / len(combined_time_profile)\n",
    "print(f\"Average time per request: {average_time:.6f} seconds\")\n",
    "\n",
    "# Range of time across all requests\n",
    "time_range = max(combined_time_profile) - min(combined_time_profile)\n",
    "print(f\"Range of time across all requests: {time_range:.6f} seconds\")\n",
    "\n",
    "# 99th percentile of request times\n",
    "percentile_99 = np.percentile(combined_time_profile, 99)\n",
    "print(f\"99th percentile of request times: {percentile_99:.6f} seconds\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08360ec-c65d-4b5b-903f-3b6c9cf1c4e8",
   "metadata": {},
   "source": [
    "---\n",
    "## Approximate Search With IVF using K-Means\n",
    "\n",
    "The solution above is fast at the current size and scale.  As the number of embeddings increase it could be helpful to search a subset of embeddings for faster responses.  A simple way to extend the brute-force search to a subset is an Inverted File (IVF) index. How?\n",
    "- Cluster the embeddings into k groups, using [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "- Create an inverted list that assigns embeddings to clusters\n",
    "- Search by first finding the closest cluster then only searching within those\n",
    "\n",
    "Here the clustering with k-means is trained with [scikit-learn `sklearn.cluster.KMeans`](https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4b84f-bacc-40ca-9b35-6993d2afecbb",
   "metadata": {},
   "source": [
    "### Cluster With k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e04f8f7-9922-45f3-9e79-b16f2c7e2ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 100\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters = k, random_state = 0)\n",
    "cluster_assignments = kmeans.fit_predict(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2bc1df-f162-4df1-ad6f-34272bf671d8",
   "metadata": {},
   "source": [
    "### Create Inverted Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b5523c6-9dd9-4387-ac1c-63b3b6048447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inverted_lists = [[] for _ in range(k)]\n",
    "for i, cluster_id in enumerate(cluster_assignments):\n",
    "    inverted_lists[cluster_id].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e7c579b4-57d8-4bb4-87d2-d2ec39c5ec5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverted_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a2739745-2687-4bc1-90af-47c388041a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[376,\n",
       " 457,\n",
       " 460,\n",
       " 465,\n",
       " 470,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 484,\n",
       " 485,\n",
       " 487]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_lists[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23365a6-6742-42e7-85b1-61649e8ea6be",
   "metadata": {},
   "source": [
    "### Search With IVF Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae444297-617a-4362-b899-2927bebd3dd9",
   "metadata": {},
   "source": [
    "#### Find Closest Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9a5fc-e194-41b1-9a6d-39add3b6b607",
   "metadata": {
    "tags": []
   },
   "source": [
    "The center of each cluster is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a5e2ff1c-7e66-4c0e-ad7c-de375c118d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 768)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "19669f55-56a0-4814-b43e-b03903ccf081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3d303b0f-f5a9-4d99-83f7-a2e116a06a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_similarity = np.dot(question_embedding, kmeans.cluster_centers_.T)\n",
    "nearest_clusters = np.argsort(cluster_similarity)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e14f1f83-52be-44c5-a8e3-7a5517d761ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([89, 58, 28,  9, 53, 17, 59, 50, 14, 30])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de18859-db89-4480-a5e5-9a92b5d6d74c",
   "metadata": {},
   "source": [
    "#### Search Within Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6ecf9dc3-48a2-4f8c-a731-0bdc91b8d5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidate_indicies = [idv for cluster_id in nearest_clusters for idv in inverted_lists[cluster_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fb4e0-65b9-4575-a11c-2b2a9bc99bb0",
   "metadata": {},
   "source": [
    "#### Top Matches Within Candidate Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "386d9caa-cb43-4c74-8d80-ac2bf3ba8433",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[27, 0.47031834864450583],\n",
       " [838, 0.47357323331558154],\n",
       " [840, 0.4883681719341052],\n",
       " [837, 0.4904491447830121],\n",
       " [29, 0.5006052048144691],\n",
       " [26, 0.5033481946111171],\n",
       " [40, 0.5126844935129918],\n",
       " [836, 0.5244194362041271],\n",
       " [36, 0.5724333016720693],\n",
       " [38, 0.5843799337008113]]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_index = vector_index[candidate_indicies]\n",
    "candidate_similarity = np.dot(question_embedding, candidate_index.T)\n",
    "ivf_matches = [[candidate_indicies[match], candidate_similarity[match]] for match in np.argsort(candidate_similarity)[-top_n:].tolist()]\n",
    "ivf_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abf5a1-f648-4ff9-9050-f96c1ee6a3fa",
   "metadata": {},
   "source": [
    "#### Compare To Top Matches From Brute-Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "de8f9c53-4e54-40c4-84af-180923abb5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[27, 0.47031834864450583],\n",
       " [838, 0.47357323331558154],\n",
       " [840, 0.4883681719341052],\n",
       " [837, 0.4904491447830121],\n",
       " [29, 0.5006052048144691],\n",
       " [26, 0.5033481946111171],\n",
       " [40, 0.5126844935129918],\n",
       " [836, 0.5244194362041271],\n",
       " [36, 0.5724333016720691],\n",
       " [38, 0.5843799337008113]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 10\n",
    "similarity = np.dot(question_embedding, vector_index.T)\n",
    "matches = np.argsort(similarity)[-top_n:].tolist()\n",
    "matches = [[match, similarity[match]] for match in matches]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9aa15578-5378-49e4-b4e5-0369dc496ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[0] for i in matches] == [i[0] for i in ivf_matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520212a-06bd-4b37-9513-1b66dbd1dd58",
   "metadata": {},
   "source": [
    "#### Put Steps Together For Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d196e5ca-ddc6-4a15-85eb-0de0177fa37a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[27, 0.47031834864450583],\n",
       " [838, 0.47357323331558154],\n",
       " [840, 0.4883681719341052],\n",
       " [837, 0.4904491447830121],\n",
       " [29, 0.5006052048144691],\n",
       " [26, 0.5033481946111171],\n",
       " [40, 0.5126844935129918],\n",
       " [836, 0.5244194362041271],\n",
       " [36, 0.5724333016720693],\n",
       " [38, 0.5843799337008113]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_clusters = np.argsort(np.dot(question_embedding, kmeans.cluster_centers_.T))[-top_c:]\n",
    "candidate_indices = np.concatenate([inverted_lists[cluster_id] for cluster_id in nearest_clusters])\n",
    "candidate_similarity = np.dot(question_embedding, vector_index[candidate_indices].T)\n",
    "top_indices = np.argsort(candidate_similarity)[-top_n:]\n",
    "ivf_matches = [[candidate_indices[i], candidate_similarity[i]] for i in top_indices]\n",
    "ivf_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617ac2c-f9b0-4967-9b39-36d26201cf58",
   "metadata": {},
   "source": [
    "### Time Sequential Operations\n",
    "\n",
    "Similar to the brute-force timing above, calculate the time for various numbers of sequential operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367f2a4-5a5f-435e-8ac8-5650e6aca433",
   "metadata": {},
   "source": [
    "#### Combined Local Process of Matching, Prompt Construction:\n",
    "\n",
    "Leave out the embeddings requests where are API wait time to the local processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2975ae4b-a99a-4e17-b57a-432b5ea86c05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for n = 1: 0.001008 seconds\n",
      "Execution time for n = 10: 0.007617 seconds\n",
      "Execution time for n = 100: 0.065586 seconds\n",
      "Execution time for n = 1000: 0.651683 seconds\n",
      "Execution time for n = 10000: 6.369548 seconds\n",
      "Execution time for n = 100000: 65.594522 seconds\n"
     ]
    }
   ],
   "source": [
    "top_c = 10 # number of clusters to match\n",
    "top_n = 10 # number of neighbors to match\n",
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "\n",
    "combined_time_ivf = []\n",
    "for x in range(6):\n",
    "    n = 10**x\n",
    "    start_time = time.time()\n",
    "    for i in range(n):\n",
    "        # get top_n matches with IVF\n",
    "        nearest_clusters = np.argsort(np.dot(question_embedding, kmeans.cluster_centers_.T))[-top_c:]\n",
    "        candidate_indices = np.concatenate([inverted_lists[cluster_id] for cluster_id in nearest_clusters])\n",
    "        candidate_similarity = np.dot(question_embedding, vector_index[candidate_indices].T)\n",
    "        top_indices = np.argsort(candidate_similarity)[-top_n:]\n",
    "        ivf_matches = [[candidate_indices[i], candidate_similarity[i]] for i in top_indices]\n",
    "        \n",
    "        # construct prompt:\n",
    "        prompt = ''\n",
    "        for m, match in enumerate(ivf_matches):\n",
    "            prompt += f\"Context {m+1}:\\n{content_chunks[vector_db[match[0]][0]]}\\n\\n\"\n",
    "        prompt += f'Answer the following question using the provided contexts:\\n{question}'        \n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time for n = {n}: {execution_time:.6f} seconds\")\n",
    "    combined_time_ivf.append(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713320db-96d7-4c24-9f84-71bd79dde005",
   "metadata": {},
   "source": [
    "Compare timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "19f98cf4-aeac-4f4e-a52c-b3836fa3d297",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1 iterations: IVF was 34.25% faster\n",
      "For 10 iterations: IVF was 56.45% faster\n",
      "For 100 iterations: IVF was 48.33% faster\n",
      "For 1000 iterations IVF was 32.48% slower\n",
      "For 10000 iterations IVF was 52.04% slower\n",
      "For 100000 iterations IVF was 46.60% slower\n"
     ]
    }
   ],
   "source": [
    "for i, (t, ivf_t) in enumerate(zip(combined_time, combined_time_ivf)):\n",
    "    adiff = abs(t-ivf_t)\n",
    "    if t <= ivf_t:\n",
    "        print(f\"For {10**i} iterations IVF was {100*(adiff/t):.2f}% slower\")\n",
    "    else:\n",
    "        print(f\"For {10**i} iterations: IVF was {100*(adiff/t):.2f}% faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635c371-8432-4729-a64f-4405a1c4995f",
   "metadata": {},
   "source": [
    "### Profile Sequential Operations Timing\n",
    "\n",
    "Now collect the individual timings for local operations and review the profile of timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3035ee6f-2a3a-4cca-8027-7d22dc35409d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "\n",
    "combined_time_profile_ivf = []\n",
    "\n",
    "n = 10000\n",
    "\n",
    "for i in range(n):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    top_c = 10 # number of clusters to match\n",
    "    top_n = 10 # number of neighbors to match\n",
    "    # get top_n matches with IVF\n",
    "    nearest_clusters = np.argsort(np.dot(question_embedding, kmeans.cluster_centers_.T))[-top_c:]\n",
    "    candidate_indices = np.concatenate([inverted_lists[cluster_id] for cluster_id in nearest_clusters])\n",
    "    candidate_similarity = np.dot(question_embedding, vector_index[candidate_indices].T)\n",
    "    top_indices = np.argsort(candidate_similarity)[-top_n:]\n",
    "    ivf_matches = [[candidate_indices[i], candidate_similarity[i]] for i in top_indices]\n",
    "\n",
    "    # construct prompt:\n",
    "    prompt = ''\n",
    "    for m, match in enumerate(ivf_matches):\n",
    "        prompt += f\"Context {m+1}:\\n{content_chunks[vector_db[match[0]][0]]}\\n\\n\"\n",
    "    prompt += f'Answer the following question using the provided contexts:\\n{question}'\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    combined_time_profile_ivf.append(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fd36d0cc-da43-4846-be72-d44b50c8ec4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for all requests: 6.331494 seconds\n",
      "Average time per request: 0.000633 seconds\n",
      "Range of time across all requests: 0.022187 seconds\n",
      "99th percentile of request times: 0.001050 seconds\n"
     ]
    }
   ],
   "source": [
    "# Total time for all requests\n",
    "total_time = sum(combined_time_profile_ivf)\n",
    "print(f\"Total time for all requests: {total_time:.6f} seconds\")\n",
    "\n",
    "# Average time per request\n",
    "average_time = total_time / len(combined_time_profile_ivf)\n",
    "print(f\"Average time per request: {average_time:.6f} seconds\")\n",
    "\n",
    "# Range of time across all requests\n",
    "time_range = max(combined_time_profile_ivf) - min(combined_time_profile_ivf)\n",
    "print(f\"Range of time across all requests: {time_range:.6f} seconds\")\n",
    "\n",
    "# 99th percentile of request times\n",
    "percentile_99 = np.percentile(combined_time_profile_ivf, 99)\n",
    "print(f\"99th percentile of request times: {percentile_99:.6f} seconds\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8fa8b5-7cf3-4059-8c6d-b64de398026d",
   "metadata": {},
   "source": [
    "Compare Timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a5942725-e6ba-4bd2-8ac1-8d31ca08fa99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- IVF -----\n",
      "Total time: 6.331494 seconds\n",
      "Average time: 0.000633 seconds\n",
      "Range: 0.022187 seconds\n",
      "99th percentile: 0.001050 seconds\n",
      "97th percentile: 0.000926 seconds\n",
      "95th percentile: 0.000889 seconds\n",
      "----- Brute Force -----\n",
      "Total time: 5.589185 seconds\n",
      "Average time: 0.000559 seconds\n",
      "Range: 0.016161 seconds\n",
      "99th percentile: 0.001382 seconds\n",
      "97th percentile: 0.000990 seconds\n",
      "95th percentile: 0.000945 seconds\n"
     ]
    }
   ],
   "source": [
    "def print_time_stats(label, times):\n",
    "    \"\"\"Prints timing statistics for a given list of times.\"\"\"\n",
    "    total_time = sum(times)\n",
    "    average_time = total_time / len(times)\n",
    "    time_range = max(times) - min(times)\n",
    "    percentile_99 = np.percentile(times, 99)\n",
    "    percentile_97 = np.percentile(times, 97)\n",
    "    percentile_95 = np.percentile(times, 95)\n",
    "    print(f\"----- {label} -----\")\n",
    "    print(f\"Total time: {total_time:.6f} seconds\")\n",
    "    print(f\"Average time: {average_time:.6f} seconds\")\n",
    "    print(f\"Range: {time_range:.6f} seconds\")\n",
    "    print(f\"99th percentile: {percentile_99:.6f} seconds\")\n",
    "    print(f\"97th percentile: {percentile_97:.6f} seconds\")\n",
    "    print(f\"95th percentile: {percentile_95:.6f} seconds\")\n",
    "    return (total_time, average_time, time_range, percentile_99, percentile_97, percentile_95)\n",
    "    \n",
    "# Print individual statistics\n",
    "results_ivf = print_time_stats(\"IVF\", combined_time_profile_ivf)\n",
    "results_bf = print_time_stats(\"Brute Force\", combined_time_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0e36af63-50c5-4c06-87bc-0c69e63d5f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the 'Total time' IVF was 13.28% slower:\n",
      "\tbrute force = 5.589185\n",
      "\tIVF = 6.331494\n",
      "For the 'Average Time' IVF was 13.28% slower:\n",
      "\tbrute force = 0.000559\n",
      "\tIVF = 0.000633\n",
      "For the 'Range' IVF was 37.28% slower:\n",
      "\tbrute force = 0.016161\n",
      "\tIVF = 0.022187\n",
      "For the '99th percentile' IVF was 23.98% faster:\n",
      "\tbrute force = 0.001382\n",
      "\tIVF = 0.001050\n",
      "For the '97th percentile' IVF was 6.43% faster:\n",
      "\tbrute force = 0.000990\n",
      "\tIVF = 0.000926\n",
      "For the '95th percentile' IVF was 5.96% faster:\n",
      "\tbrute force = 0.000945\n",
      "\tIVF = 0.000889\n"
     ]
    }
   ],
   "source": [
    "metrics = {'1' : 'Total time', '2' : 'Average Time', '3' : 'Range', '4' : '99th percentile', '5':'97th percentile', '6':'95th percentile'}\n",
    "for i, (bf, ivf) in enumerate(zip(results_bf, results_ivf)):\n",
    "    adiff = abs(bf-ivf)\n",
    "    if bf <= ivf:\n",
    "        print(f\"For the '{metrics[str(i+1)]}' IVF was {100*(adiff/bf):.2f}% slower:\\n\\tbrute force = {bf:.6f}\\n\\tIVF = {ivf:.6f}\")\n",
    "    else:\n",
    "        print(f\"For the '{metrics[str(i+1)]}' IVF was {100*(adiff/bf):.2f}% faster:\\n\\tbrute force = {bf:.6f}\\n\\tIVF = {ivf:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebbd14a-bc67-4d3a-8c1d-5c8cd421871b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
